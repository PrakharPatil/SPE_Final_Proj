schema: '2.0'
stages:
  data_import:
    cmd: python scripts/data_import.py
    deps:
    - path: scripts/data_import.py
      hash: md5
      md5: b769ba85323e5a9096d855636c677b98
      size: 3994
    params:
      params.yaml:
        data_import.file_size: 100
    outs:
    - path: Data/Raw
      hash: md5
      md5: e77eeb0fd95f1391b2fa3ffcb28d71f8.dir
      size: 421221679
      nfiles: 4
  data_preprocessing:
    cmd: python scripts/data_preprocessing.py
    deps:
    - path: Data/Raw
      hash: md5
      md5: e77eeb0fd95f1391b2fa3ffcb28d71f8.dir
      size: 421221679
      nfiles: 4
    - path: scripts/data_preprocessing.py
      hash: md5
      md5: 1ba5afa04a71188f4a734b297ded4b5f
      size: 3457
    outs:
    - path: Data/Clean
      hash: md5
      md5: c9a460fbc088f015854ed63bd02668cd.dir
      size: 419304588
      nfiles: 4
  tokenizer:
    cmd: python scripts/tokenizer.py
    deps:
    - path: Data/Clean
      hash: md5
      md5: c9a460fbc088f015854ed63bd02668cd.dir
      size: 419304588
      nfiles: 4
    - path: scripts/tokenizer.py
      hash: md5
      md5: 803b7b991a8da20c9cd40327b1f59f73
      size: 5356
    params:
      params.yaml:
        tokenizer.vocab_size: 2500
    outs:
    - path: Joblibs/tokenizer.joblib
      hash: md5
      md5: 2da412b290cec4f629ae5389701da985
      size: 63517
