schema: '2.0'
stages:
  data_import:
    cmd: python scripts/data_import.py
    deps:
    - path: scripts/data_import.py
      hash: md5
      md5: f7fb1791a39209ff4ea1cb3a28bdec34
      size: 3980
    params:
      params.yaml:
        data_import.file_size: 75
    outs:
    - path: Data/Raw
      hash: md5
      md5: 7baa170d56db8a5af484fb0e91b49a88.dir
      size: 22444
      nfiles: 4
  data_preprocessing:
    cmd: python scripts/data_preprocessing.py
    deps:
    - path: Data/Raw
      hash: md5
      md5: 7baa170d56db8a5af484fb0e91b49a88.dir
      size: 22444
      nfiles: 4
    - path: scripts/data_preprocessing.py
      hash: md5
      md5: 1ba5afa04a71188f4a734b297ded4b5f
      size: 3457
    outs:
    - path: Data/Clean
      hash: md5
      md5: 95fc9b6339617d440f68618c6883804e.dir
      size: 22359
      nfiles: 4
  tokenizer:
    cmd: python scripts/tokenizer.py
    deps:
    - path: Data/Clean
      hash: md5
      md5: 95fc9b6339617d440f68618c6883804e.dir
      size: 22359
      nfiles: 4
    - path: scripts/tokenizer.py
      hash: md5
      md5: 803b7b991a8da20c9cd40327b1f59f73
      size: 5356
    params:
      params.yaml:
        tokenizer.vocab_size: 2500
    outs:
    - path: Joblibs/tokenizer.joblib
      hash: md5
      md5: 5be6c6300dbf3c6e2da70e10e92478cc
      size: 69257
