{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:19.162365Z",
     "start_time": "2025-05-13T11:18:19.155451Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)  # 👈 force reload if values were already set\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:19.226859Z",
     "start_time": "2025-05-13T11:18:19.224876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "# Go up two levels\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:19.355179Z",
     "start_time": "2025-05-13T11:18:19.352857Z"
    }
   },
   "source": [
    "import os\n",
    "print(os.getenv('DB_HOST'), os.getenv('DB_USER'), os.getenv('DB_PASSWORD'), os.getenv('DB_NAME'))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 root MySQL31# text_dataset_db\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:27.673106Z",
     "start_time": "2025-05-13T11:18:20.021576Z"
    }
   },
   "source": [
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# ✅ Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Fetch database configuration from environment variables\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'database': os.getenv('DB_NAME')\n",
    "}\n",
    "\n",
    "# ✅ Connect to MySQL using PyMySQL\n",
    "conn = pymysql.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ✅ Define a function to fetch content based on the level\n",
    "def fetch_data_by_level(level):\n",
    "    query = \"SELECT content FROM text_data WHERE level = %s\"\n",
    "    cursor.execute(query, (level,))\n",
    "    rows = cursor.fetchall()\n",
    "    return [row[0] for row in rows]\n",
    "\n",
    "# ✅ Fetch data for each level\n",
    "L1= fetch_data_by_level('L1')\n",
    "L2 = fetch_data_by_level('L2')\n",
    "L3 = fetch_data_by_level('L3')\n",
    "L4 = fetch_data_by_level('L4')\n",
    "\n",
    "# ✅ Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# ✅ Print samples (optional)\n",
    "print(\"L1 Data Sample:\", L1[0][:5000] if L1 else \"No data found\")\n",
    "print(\"L2 Data Sample:\", L2[0][:500] if L2 else \"No data found\")\n",
    "print(\"L3 Data Sample:\", L3[0][:500] if L3 else \"No data found\")\n",
    "print(\"L4 Data Sample:\", L4[0][:5000] if L4 else \"No data found\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Data Sample:  Once upon a time in the land of Policymia, there lived two leaders named Majora and Minoro. Their job was to make sure all the citizens had beautiful parks, clean water, and top-notch schools. But there were so many things to fix! How would they ever decide where to start?\n",
      "L2 Data Sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "L3 Data Sample: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n",
      "L4 Data Sample: Research on Neural Machine Translation Model\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:28.400214Z",
     "start_time": "2025-05-13T11:18:27.717861Z"
    }
   },
   "source": [
    "# Concatenate all content from L1, L2, L3, L4 into single strings\n",
    "L1_combined = \" \".join(L1) if L1 else \"\"\n",
    "L2_combined = \" \".join(L2) if L2 else \"\"\n",
    "L3_combined = \" \".join(L3) if L3 else \"\"\n",
    "L4_combined = \" \".join(L4) if L4 else \"\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:28.491052Z",
     "start_time": "2025-05-13T11:18:28.484029Z"
    }
   },
   "source": [
    "print(\"L1 Data Sample:\", L1_combined[:500] )\n",
    "print(\"L2 Data Sample:\", L2_combined[:500] )\n",
    "print(\"L3 Data Sample:\", L3_combined[:500] )\n",
    "print(\"L4 Data Sample:\", L4_combined[:500] )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Data Sample:  Once upon a time in the land of Policymia, there lived two leaders named Majora and Minoro. Their job was to make sure all the citizens had beautiful parks, clean water, and top-notch schools. But there were so many things to fix! How would they ever decide where to start?  Majora, being the wise leader she was, knew just what to do. She invited her fellow policymakers for a big meeting at the Roundtable of Representatives. There, they discussed the most important problems Policymia faced. This\n",
      "L2 Data Sample: usually , he would be tearing around the living room , playing with his toys . but just one look at a minion sent him practically catatonic . that had been megan 's plan when she got him dressed earlier . he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older . she liked to think being surrounded by adults and older kids was one reason why he was a such a \n",
      "L3 Data Sample: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n",
      "L4 Data Sample: Research on Neural Machine Translation Model  In neural machine translation (NMT), cyclic neural networks, especially long-term and short-term memory networks and gated recurrent neural networks, have been regarded as the latest methods for sequence modeling and transduction problems for a long time, such as language modeling and machine translation. When the cyclic neural network is running, the sequence information is processed one by one, strictly following the order from left to right or fro\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:30.724350Z",
     "start_time": "2025-05-13T11:18:30.719908Z"
    }
   },
   "source": [
    "# import requests\n",
    "\n",
    "# with open(\"Data/L1_ChildrenStories.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L1 = f.read()\n",
    "# with open(\"Data/L2_BookCorpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L2 = f.read()\n",
    "# with open(\"Data/L3_CNN_DailyMail.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L3 = f.read()\n",
    "# with open(\"Data/L4_S2ORC.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L4 = f.read()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:18:30.892578Z",
     "start_time": "2025-05-13T11:18:30.890999Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:20:32.717632Z",
     "start_time": "2025-05-13T11:18:34.517428Z"
    }
   },
   "source": [
    "!pip install emoji\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Define allowed character sets\n",
    "english_regex = r\"[a-zA-Z0-9\\s]\"                   # English letters, numbers, spaces\n",
    "math_symbols  = r\"[\\+\\-\\*/=<>∑∫√πθΣ∂∞]\"             # Add more math symbols if needed\n",
    "special_chars = r\"[\\.,!?;:'\\\"()\\[\\]{}#@%^&*_~]\"     # Common special characters\n",
    "\n",
    "# Function to clean a given text\n",
    "def clean_text(text):\n",
    "    return \"\".join(\n",
    "        c for c in text\n",
    "        if re.match(english_regex, c) or \n",
    "           re.match(math_symbols, c) or \n",
    "           re.match(special_chars, c) or \n",
    "           emoji.is_emoji(c)\n",
    "    )\n",
    "\n",
    "# Apply cleaning to all levels\n",
    "L1_cleaned = clean_text(L1)\n",
    "L2_cleaned = clean_text(L2)\n",
    "L3_cleaned = clean_text(L3)\n",
    "L4_cleaned = clean_text(L4)\n",
    "# L1_cleaned = clean_text(L1_combined)\n",
    "# L2_cleaned = clean_text(L2_combined)\n",
    "# L3_cleaned = clean_text(L3_combined)\n",
    "# L4_cleaned = clean_text(L4_combined)\n",
    "print(\"All levels cleaned and stored in *_cleaned variables!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (2.14.1)\r\n",
      "All levels cleaned and stored in *_cleaned variables!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:20:32.756647Z",
     "start_time": "2025-05-13T11:20:32.755308Z"
    }
   },
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:20:34.395899Z",
     "start_time": "2025-05-13T11:20:32.787065Z"
    }
   },
   "source": [
    "full_text = L1_cleaned+L2_cleaned+L3_cleaned+L4_cleaned\n",
    "D = len(full_text)\n",
    "D"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419268673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:21:12.112387Z",
     "start_time": "2025-05-13T11:20:34.415685Z"
    }
   },
   "source": [
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "token = full_text.encode(\"utf-8\")\n",
    "def get_stats(ids):\n",
    "    counts = {} # Creates an empty Dictionary\n",
    "    for pair in zip(ids,ids[1:]):\n",
    "        counts[pair] = counts.get(pair ,0)+1\n",
    "    return counts\n",
    "print(vocab_size)\n",
    "print(''.join(chars))\n",
    "stats = get_stats(token)\n",
    "top_pair = max(stats,key=stats.get)\n",
    "top_pair"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "\t\n",
      " !\"#%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz{}~ ©®Σθπ  ™↔↘↪∂∑√∞∫▪▫▶☀☁☄☺♀♣♥♦♻⚙⚠⚡✏✔✨❤🌈🌍🌏🌐🌞🌟🌬🌱🌳🌸🌻🌿🍃🍇🍊🍞🍡🍭🎉🎮🎶🏠🏡🏰🏼🏽🐇🐛🐦🐰🐿👑👧👨💁💎💕💖💚💡💪💻💾📃📊📚📝📡📣📱📲🔒🔗🔬🕊🖥🗣😁😂😃😄😊😎😐😔😢😮🚀🚧🚯🤖🤩🤯🥗🥦🦉🦋🦝\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:24:17.954578Z",
     "start_time": "2025-05-13T11:21:13.584010Z"
    }
   },
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "\n",
    "# Your cleaned text data\n",
    "docs = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]\n",
    "\n",
    "# Break large documents into smaller lines/chunks\n",
    "def chunked_docs():\n",
    "    for doc in docs:\n",
    "        # You can tweak the split here (e.g., '. ' or '\\n' or custom logic)\n",
    "        for line in doc.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield line\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "# Trainer with manageable vocab size\n",
    "trainer = BpeTrainer(vocab_size=2000, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "# Train using iterator to save memory\n",
    "tokenizer.train_from_iterator(chunked_docs(), trainer=trainer)\n",
    "\n",
    "# Check final vocab size\n",
    "print(\"Actual vocab size:\", tokenizer.get_vocab_size())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Actual vocab size: 2000\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:24:22.931731Z",
     "start_time": "2025-05-13T11:24:22.923934Z"
    }
   },
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(token_ids: list[int]) -> str:\n",
    "    return tokenizer.decode(token_ids)\n",
    "sample = \"math is beautiful ✨\"\n",
    "ids = encode(sample)\n",
    "print(\"→\", ids)\n",
    "print(\"←\", decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ [651, 69, 246, 1375, 124, 120, 152, 98]\n",
      "←  math is beautiful ✨\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Like Transformer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T15:10:45.672975Z",
     "start_time": "2025-05-12T14:41:12.264350Z"
    }
   },
   "source": [
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from TransformerMS.Model.model_architecture import GPTLanguageModel\n",
    "from torch.optim import AdamW\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "# Configuration (expanded with missing parameters)\n",
    "class Config:\n",
    "    batch_size = 16\n",
    "    block_size = 32\n",
    "    max_iters = 5000\n",
    "    eval_interval = 100\n",
    "    learning_rate = 1e-3\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    eval_iters = 200\n",
    "    n_embd = 64\n",
    "    n_head = 4\n",
    "    n_layer = 4\n",
    "    dropout = 0.0\n",
    "    checkpoint_dir = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "\n",
    "\n",
    "# ------------------ Training Utilities ------------------\n",
    "def get_batch_transformer(data):\n",
    "    ix = torch.randint(len(data) - Config.block_size, (Config.batch_size,))\n",
    "    x = torch.stack([data[i:i+Config.block_size] for i in ix]).to(Config.device)\n",
    "    y = torch.stack([data[i+1:i+Config.block_size+1] for i in ix]).to(Config.device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_transformer(model, train_data, val_data):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, data in zip(['train','val'], [train_data, val_data]):\n",
    "        losses = []\n",
    "        for _ in range(Config.eval_iters):\n",
    "            X, Y = get_batch_transformer(data)\n",
    "            _, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))\n",
    "\n",
    "# ------------------ Training Pipeline ------------------\n",
    "all_metrics = {\n",
    "    \"level\": [], \"iters\": [],\n",
    "    \"train_loss\": [], \"val_loss\": [],\n",
    "    \"train_ppl\": [], \"val_ppl\": []\n",
    "}\n",
    "def incremental_training_pipeline(level_texts, model, tokenizer):\n",
    "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # all_metrics = {\n",
    "    #     \"level\": [], \"iters\": [],\n",
    "    #     \"train_loss\": [], \"val_loss\": [],\n",
    "    #     \"train_ppl\": [], \"val_ppl\": []\n",
    "    # }\n",
    "\n",
    "    for level_i, level_text in enumerate(level_texts, start=1):\n",
    "        print(f\"\\n=== LEVEL {level_i} TRAINING ===\")\n",
    "        \n",
    "        # Encode text using tokenizer\n",
    "        data_ids = torch.tensor(tokenizer.encode(level_text).ids, \n",
    "                    dtype=torch.long).to(Config.device)\n",
    "        \n",
    "        # Train/validation split\n",
    "        n = int(0.9 * data_ids.size(0))\n",
    "        train_data = data_ids[:n]\n",
    "        val_data = data_ids[n:]\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.learning_rate)\n",
    "        \n",
    "        for it in range(Config.max_iters):\n",
    "            if it % Config.eval_interval == 0:\n",
    "                losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "                ppl_train = compute_perplexity(losses['train'])\n",
    "                ppl_val = compute_perplexity(losses['val'])\n",
    "\n",
    "                # Update metrics\n",
    "                all_metrics[\"level\"].append(level_i)\n",
    "                all_metrics[\"iters\"].append(it)\n",
    "                all_metrics[\"train_loss\"].append(losses['train'])\n",
    "                all_metrics[\"val_loss\"].append(losses['val'])\n",
    "                all_metrics[\"train_ppl\"].append(ppl_train.item())\n",
    "                all_metrics[\"val_ppl\"].append(ppl_val.item())\n",
    "\n",
    "                print(f\"Iter {it:4d} | Train loss {losses['train']:.4f} (ppl {ppl_train:.2f}) | \"\n",
    "                      f\"Val loss {losses['val']:.4f} (ppl {ppl_val:.2f})\")\n",
    "\n",
    "            # Training step\n",
    "            xb, yb = get_batch_transformer(train_data)\n",
    "            _, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Final evaluation\n",
    "        losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "        print(f\"\\nLevel {level_i} complete:\")\n",
    "        print(f\"Final train loss: {losses['train']:.4f}\")\n",
    "        print(f\"Final val loss:   {losses['val']:.4f}\")\n",
    "\n",
    "        # Generate sample\n",
    "        context = torch.zeros((1,1), dtype=torch.long, device=Config.device)\n",
    "        sample_ids = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "        print(\"\\nSample:\", tokenizer.decode(sample_ids))\n",
    "\n",
    "        # Save checkpoint\n",
    "        cp = os.path.join(Config.checkpoint_dir, f\"model_level{level_i}.pt\")\n",
    "        torch.save(model.state_dict(), cp)\n",
    "        print(f\"Saved checkpoint: {cp}\")\n",
    "\n",
    "    return model, all_metrics\n",
    "\n",
    "# ------------------ Main Execution ------------------\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = joblib.load(os.path.join(BASE_DIR, 'Joblibs', 'tokenizer.joblib'))  # Your trained tokenizer\n",
    "level_texts = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]  # Your data\n",
    "\n",
    "# Initialize model with proper configuration\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    block_size=Config.block_size,\n",
    "    n_embd=Config.n_embd,\n",
    "    n_head=Config.n_head,\n",
    "    n_layer=Config.n_layer,\n",
    "    dropout=Config.dropout\n",
    ").to(Config.device)\n",
    "\n",
    "# Run training\n",
    "trained_model, metrics = incremental_training_pipeline(level_texts, model, tokenizer)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LEVEL 1 TRAINING ===\n",
      "Iter    0 | Train loss 7.6107 (ppl 2019.76) | Val loss 7.6109 (ppl 2020.14)\n",
      "Iter  100 | Train loss 6.3805 (ppl 590.25) | Val loss 6.3789 (ppl 589.27)\n",
      "Iter  200 | Train loss 5.7230 (ppl 305.83) | Val loss 5.7148 (ppl 303.32)\n",
      "Iter  300 | Train loss 5.3166 (ppl 203.69) | Val loss 5.2860 (ppl 197.55)\n",
      "Iter  400 | Train loss 5.0453 (ppl 155.29) | Val loss 5.0544 (ppl 156.71)\n",
      "Iter  500 | Train loss 4.8692 (ppl 130.21) | Val loss 4.8632 (ppl 129.44)\n",
      "Iter  600 | Train loss 4.7544 (ppl 116.09) | Val loss 4.7577 (ppl 116.48)\n",
      "Iter  700 | Train loss 4.6677 (ppl 106.45) | Val loss 4.6510 (ppl 104.69)\n",
      "Iter  800 | Train loss 4.5936 (ppl 98.85) | Val loss 4.5693 (ppl 96.48)\n",
      "Iter  900 | Train loss 4.5283 (ppl 92.60) | Val loss 4.5314 (ppl 92.89)\n",
      "Iter 1000 | Train loss 4.4826 (ppl 88.47) | Val loss 4.4756 (ppl 87.85)\n",
      "Iter 1100 | Train loss 4.4386 (ppl 84.66) | Val loss 4.4518 (ppl 85.78)\n",
      "Iter 1200 | Train loss 4.3921 (ppl 80.81) | Val loss 4.3977 (ppl 81.27)\n",
      "Iter 1300 | Train loss 4.3457 (ppl 77.15) | Val loss 4.3456 (ppl 77.14)\n",
      "Iter 1400 | Train loss 4.2924 (ppl 73.15) | Val loss 4.3111 (ppl 74.52)\n",
      "Iter 1500 | Train loss 4.2521 (ppl 70.25) | Val loss 4.2538 (ppl 70.37)\n",
      "Iter 1600 | Train loss 4.2337 (ppl 68.97) | Val loss 4.2076 (ppl 67.19)\n",
      "Iter 1700 | Train loss 4.2294 (ppl 68.68) | Val loss 4.2039 (ppl 66.95)\n",
      "Iter 1800 | Train loss 4.1672 (ppl 64.54) | Val loss 4.1823 (ppl 65.52)\n",
      "Iter 1900 | Train loss 4.1520 (ppl 63.56) | Val loss 4.1591 (ppl 64.01)\n",
      "Iter 2000 | Train loss 4.1101 (ppl 60.95) | Val loss 4.1026 (ppl 60.49)\n",
      "Iter 2100 | Train loss 4.1171 (ppl 61.38) | Val loss 4.0990 (ppl 60.28)\n",
      "Iter 2200 | Train loss 4.0777 (ppl 59.01) | Val loss 4.0910 (ppl 59.80)\n",
      "Iter 2300 | Train loss 4.0380 (ppl 56.71) | Val loss 4.0512 (ppl 57.47)\n",
      "Iter 2400 | Train loss 4.0438 (ppl 57.04) | Val loss 4.0420 (ppl 56.94)\n",
      "Iter 2500 | Train loss 4.0135 (ppl 55.34) | Val loss 4.0211 (ppl 55.76)\n",
      "Iter 2600 | Train loss 3.9932 (ppl 54.23) | Val loss 4.0216 (ppl 55.79)\n",
      "Iter 2700 | Train loss 3.9914 (ppl 54.13) | Val loss 3.9652 (ppl 52.73)\n",
      "Iter 2800 | Train loss 3.9828 (ppl 53.67) | Val loss 3.9827 (ppl 53.66)\n",
      "Iter 2900 | Train loss 3.9496 (ppl 51.92) | Val loss 3.9661 (ppl 52.78)\n",
      "Iter 3000 | Train loss 3.9477 (ppl 51.82) | Val loss 3.9620 (ppl 52.56)\n",
      "Iter 3100 | Train loss 3.9425 (ppl 51.55) | Val loss 3.9378 (ppl 51.30)\n",
      "Iter 3200 | Train loss 3.9057 (ppl 49.68) | Val loss 3.9296 (ppl 50.89)\n",
      "Iter 3300 | Train loss 3.8874 (ppl 48.79) | Val loss 3.8929 (ppl 49.05)\n",
      "Iter 3400 | Train loss 3.8799 (ppl 48.42) | Val loss 3.8909 (ppl 48.95)\n",
      "Iter 3500 | Train loss 3.8947 (ppl 49.14) | Val loss 3.9218 (ppl 50.49)\n",
      "Iter 3600 | Train loss 3.8894 (ppl 48.88) | Val loss 3.8764 (ppl 48.25)\n",
      "Iter 3700 | Train loss 3.8294 (ppl 46.04) | Val loss 3.8460 (ppl 46.81)\n",
      "Iter 3800 | Train loss 3.8443 (ppl 46.73) | Val loss 3.8510 (ppl 47.04)\n",
      "Iter 3900 | Train loss 3.8574 (ppl 47.34) | Val loss 3.8517 (ppl 47.07)\n",
      "Iter 4000 | Train loss 3.8406 (ppl 46.55) | Val loss 3.8460 (ppl 46.81)\n",
      "Iter 4100 | Train loss 3.8209 (ppl 45.65) | Val loss 3.8208 (ppl 45.64)\n",
      "Iter 4200 | Train loss 3.8137 (ppl 45.32) | Val loss 3.8153 (ppl 45.39)\n",
      "Iter 4300 | Train loss 3.8273 (ppl 45.94) | Val loss 3.8425 (ppl 46.64)\n",
      "Iter 4400 | Train loss 3.7844 (ppl 44.01) | Val loss 3.7751 (ppl 43.60)\n",
      "Iter 4500 | Train loss 3.7898 (ppl 44.25) | Val loss 3.7757 (ppl 43.63)\n",
      "Iter 4600 | Train loss 3.7911 (ppl 44.30) | Val loss 3.7911 (ppl 44.31)\n",
      "Iter 4700 | Train loss 3.7508 (ppl 42.55) | Val loss 3.7742 (ppl 43.56)\n",
      "Iter 4800 | Train loss 3.7682 (ppl 43.30) | Val loss 3.7844 (ppl 44.01)\n",
      "Iter 4900 | Train loss 3.7659 (ppl 43.20) | Val loss 3.7557 (ppl 42.76)\n",
      "\n",
      "Level 1 complete:\n",
      "Final train loss: 3.7429\n",
      "Final val loss:   3.7555\n",
      "\n",
      "Sample:   Aoilly, a wondererful restive led, machine crant musching people if they learned the best sticking sale million cenest land filled with noise arriving road.\"  Mrs. Gox smiled thoughtfully headom excitedly, \"Fari, Bango, something called out of dight. Mixeling out at Fortunece did it, not significant mouths a sudden twist. perfessorse helps emerged tiny, he discovered that Fishing pokey's connection to triord from her to help. Although couldn't emball, changes were preparing the cleanent, they saw a total. He told be they needed to multislex ways to talk to already, but unfired their journey, the Pitunion understood that covertime teacher brings into preserving things, still incredible colors conversity. A love is equocation between friendship and balance. Although, they continued their journey only sorts in School, sometimes they possibilities, they wishing her new thoughts to availored people can renewable views with each librace during their world. One glious network of lining her toy bud away, OFE. Alvontigued, Binuffy and Augaryl feed their roading luckled around as it was surfound their imashedacation!\"  Technology, Benny and Twupped towards Mr. They loved exploring their chlor. They both countries noticed something organized by each other things and they came, which he learned a valuable lesson about communities and readen. However, despite there, understanding people staying temposting brats gave everyone in surprising the process, flursuing for protecting the issues, ensurement of education, individuals exped her studying, causing itself.  On their millage, Pentalfinding her way, he raised her, their thunderstination and pinkling, they realized that nature seem suited one steps to adapt\n",
      "Saved checkpoint: /Users/prakhar_patil/Desktop/SPE_Proj/TransformerMS/checkpoints/model_level1.pt\n",
      "\n",
      "=== LEVEL 2 TRAINING ===\n",
      "Iter    0 | Train loss 6.5994 (ppl 734.62) | Val loss 6.7428 (ppl 847.94)\n",
      "Iter  100 | Train loss 4.4087 (ppl 82.16) | Val loss 4.4059 (ppl 81.93)\n",
      "Iter  200 | Train loss 4.2826 (ppl 72.43) | Val loss 4.2722 (ppl 71.68)\n",
      "Iter  300 | Train loss 4.1817 (ppl 65.48) | Val loss 4.1987 (ppl 66.60)\n",
      "Iter  400 | Train loss 4.1247 (ppl 61.85) | Val loss 4.1486 (ppl 63.35)\n",
      "Iter  500 | Train loss 4.1083 (ppl 60.84) | Val loss 4.1077 (ppl 60.81)\n",
      "Iter  600 | Train loss 4.0909 (ppl 59.79) | Val loss 4.1179 (ppl 61.43)\n",
      "Iter  700 | Train loss 4.0330 (ppl 56.43) | Val loss 4.0526 (ppl 57.54)\n",
      "Iter  800 | Train loss 4.0246 (ppl 55.96) | Val loss 4.0514 (ppl 57.48)\n",
      "Iter  900 | Train loss 3.9995 (ppl 54.57) | Val loss 4.0194 (ppl 55.67)\n",
      "Iter 1000 | Train loss 3.9889 (ppl 54.00) | Val loss 4.0267 (ppl 56.07)\n",
      "Iter 1100 | Train loss 3.9628 (ppl 52.61) | Val loss 3.9949 (ppl 54.32)\n",
      "Iter 1200 | Train loss 3.9572 (ppl 52.31) | Val loss 3.9761 (ppl 53.31)\n",
      "Iter 1300 | Train loss 3.9576 (ppl 52.33) | Val loss 3.9787 (ppl 53.45)\n",
      "Iter 1400 | Train loss 3.9277 (ppl 50.79) | Val loss 3.9698 (ppl 52.97)\n",
      "Iter 1500 | Train loss 3.9109 (ppl 49.94) | Val loss 3.9332 (ppl 51.07)\n",
      "Iter 1600 | Train loss 3.9220 (ppl 50.50) | Val loss 3.9585 (ppl 52.38)\n",
      "Iter 1700 | Train loss 3.9087 (ppl 49.83) | Val loss 3.9430 (ppl 51.57)\n",
      "Iter 1800 | Train loss 3.8716 (ppl 48.02) | Val loss 3.9344 (ppl 51.13)\n",
      "Iter 1900 | Train loss 3.8756 (ppl 48.21) | Val loss 3.9296 (ppl 50.89)\n",
      "Iter 2000 | Train loss 3.8573 (ppl 47.34) | Val loss 3.9145 (ppl 50.12)\n",
      "Iter 2100 | Train loss 3.8925 (ppl 49.04) | Val loss 3.9061 (ppl 49.70)\n",
      "Iter 2200 | Train loss 3.8470 (ppl 46.85) | Val loss 3.8692 (ppl 47.90)\n",
      "Iter 2300 | Train loss 3.8568 (ppl 47.31) | Val loss 3.9054 (ppl 49.67)\n",
      "Iter 2400 | Train loss 3.8406 (ppl 46.55) | Val loss 3.8963 (ppl 49.22)\n",
      "Iter 2500 | Train loss 3.8134 (ppl 45.30) | Val loss 3.8872 (ppl 48.77)\n",
      "Iter 2600 | Train loss 3.8407 (ppl 46.56) | Val loss 3.8678 (ppl 47.83)\n",
      "Iter 2700 | Train loss 3.8422 (ppl 46.63) | Val loss 3.8656 (ppl 47.73)\n",
      "Iter 2800 | Train loss 3.8388 (ppl 46.47) | Val loss 3.8560 (ppl 47.28)\n",
      "Iter 2900 | Train loss 3.7968 (ppl 44.56) | Val loss 3.8504 (ppl 47.01)\n",
      "Iter 3000 | Train loss 3.8209 (ppl 45.64) | Val loss 3.8550 (ppl 47.23)\n",
      "Iter 3100 | Train loss 3.7948 (ppl 44.47) | Val loss 3.8310 (ppl 46.11)\n",
      "Iter 3200 | Train loss 3.7975 (ppl 44.59) | Val loss 3.8366 (ppl 46.37)\n",
      "Iter 3300 | Train loss 3.8165 (ppl 45.45) | Val loss 3.8597 (ppl 47.45)\n",
      "Iter 3400 | Train loss 3.7979 (ppl 44.61) | Val loss 3.8347 (ppl 46.28)\n",
      "Iter 3500 | Train loss 3.7890 (ppl 44.21) | Val loss 3.8391 (ppl 46.48)\n",
      "Iter 3600 | Train loss 3.7851 (ppl 44.04) | Val loss 3.8253 (ppl 45.85)\n",
      "Iter 3700 | Train loss 3.7770 (ppl 43.68) | Val loss 3.8260 (ppl 45.88)\n",
      "Iter 3800 | Train loss 3.7585 (ppl 42.89) | Val loss 3.8046 (ppl 44.91)\n",
      "Iter 3900 | Train loss 3.7833 (ppl 43.96) | Val loss 3.8332 (ppl 46.21)\n",
      "Iter 4000 | Train loss 3.7501 (ppl 42.52) | Val loss 3.8115 (ppl 45.22)\n",
      "Iter 4100 | Train loss 3.7715 (ppl 43.45) | Val loss 3.8075 (ppl 45.04)\n",
      "Iter 4200 | Train loss 3.7731 (ppl 43.51) | Val loss 3.8121 (ppl 45.25)\n",
      "Iter 4300 | Train loss 3.7701 (ppl 43.38) | Val loss 3.8110 (ppl 45.20)\n",
      "Iter 4400 | Train loss 3.7549 (ppl 42.73) | Val loss 3.8107 (ppl 45.18)\n",
      "Iter 4500 | Train loss 3.7753 (ppl 43.61) | Val loss 3.8024 (ppl 44.81)\n",
      "Iter 4600 | Train loss 3.7440 (ppl 42.27) | Val loss 3.8045 (ppl 44.90)\n",
      "Iter 4700 | Train loss 3.7562 (ppl 42.78) | Val loss 3.7944 (ppl 44.45)\n",
      "Iter 4800 | Train loss 3.7227 (ppl 41.38) | Val loss 3.7909 (ppl 44.30)\n",
      "Iter 4900 | Train loss 3.7382 (ppl 42.02) | Val loss 3.8017 (ppl 44.78)\n",
      "\n",
      "Level 2 complete:\n",
      "Final train loss: 3.7360\n",
      "Final val loss:   3.7818\n",
      "\n",
      "Sample:  for any word at the triadies after each other 's enticious perient a crond swallicking out in a pure lu- yet some of them will shield them . i turn i normally to decide and never smiled and accusive ! i hated giving the worling mouth for everything was . they got the other thing i was their way of blond body together again . '' he paid it soft and her smile , to the water with him .  look , you has chately been micael . ''  this time , that gods just blood . see maro has expected max , '' she walked back to the vicultures of my lip , not recentledge .  start brideing the night ; that , you have a group of friendship , i 'd last short against the security that ! i dry not . i 'm here . '' i guessed i 'm not my cuse and shoved my face away from my intimic and not have them up my shoulder , with a bull of my hand . never having me far lane me like a idea and he 's under your tax . heavy bembled in the car and not say to him , orture . you run into a minillionaired dial , you . or me taken her in my pitching in your room in it below under the pago with you . it 's really showing your trutter at why this endively she had my mother has used to family in my dark part in the message . you 'd told him that my head had happened here it . so when you am not joos curves me in the jarc on your bed facern . '' the thought of relief ... she heard him laughter , she leaned inside to admir word more , and he jerked to put her on . the table was near him . no she 'd n't already lay night on the windowcent behind of tasting tension of pounding and try the hundred . ''  do not matter how we know . do me mad kindling for you home , twice . i think we are pank . '' godopereated as if he\n",
      "Saved checkpoint: /Users/prakhar_patil/Desktop/SPE_Proj/TransformerMS/checkpoints/model_level2.pt\n",
      "\n",
      "=== LEVEL 3 TRAINING ===\n",
      "Iter    0 | Train loss 6.7875 (ppl 886.73) | Val loss 6.7396 (ppl 845.20)\n",
      "Iter  100 | Train loss 4.7068 (ppl 110.70) | Val loss 4.7203 (ppl 112.20)\n",
      "Iter  200 | Train loss 4.5560 (ppl 95.20) | Val loss 4.5741 (ppl 96.94)\n",
      "Iter  300 | Train loss 4.4716 (ppl 87.49) | Val loss 4.5176 (ppl 91.61)\n",
      "Iter  400 | Train loss 4.4300 (ppl 83.93) | Val loss 4.4518 (ppl 85.78)\n",
      "Iter  500 | Train loss 4.3792 (ppl 79.78) | Val loss 4.4039 (ppl 81.77)\n",
      "Iter  600 | Train loss 4.3528 (ppl 77.69) | Val loss 4.3853 (ppl 80.26)\n",
      "Iter  700 | Train loss 4.3221 (ppl 75.35) | Val loss 4.3467 (ppl 77.22)\n",
      "Iter  800 | Train loss 4.3178 (ppl 75.02) | Val loss 4.3418 (ppl 76.85)\n",
      "Iter  900 | Train loss 4.2698 (ppl 71.51) | Val loss 4.3205 (ppl 75.22)\n",
      "Iter 1000 | Train loss 4.2543 (ppl 70.41) | Val loss 4.2921 (ppl 73.12)\n",
      "Iter 1100 | Train loss 4.2641 (ppl 71.10) | Val loss 4.3029 (ppl 73.91)\n",
      "Iter 1200 | Train loss 4.2543 (ppl 70.41) | Val loss 4.2754 (ppl 71.91)\n",
      "Iter 1300 | Train loss 4.2402 (ppl 69.42) | Val loss 4.2593 (ppl 70.76)\n",
      "Iter 1400 | Train loss 4.2078 (ppl 67.21) | Val loss 4.2621 (ppl 70.96)\n",
      "Iter 1500 | Train loss 4.1904 (ppl 66.05) | Val loss 4.2614 (ppl 70.91)\n",
      "Iter 1600 | Train loss 4.1851 (ppl 65.70) | Val loss 4.2426 (ppl 69.59)\n",
      "Iter 1700 | Train loss 4.1739 (ppl 64.97) | Val loss 4.2012 (ppl 66.76)\n",
      "Iter 1800 | Train loss 4.1715 (ppl 64.82) | Val loss 4.2141 (ppl 67.63)\n",
      "Iter 1900 | Train loss 4.1643 (ppl 64.35) | Val loss 4.2041 (ppl 66.96)\n",
      "Iter 2000 | Train loss 4.1497 (ppl 63.41) | Val loss 4.2071 (ppl 67.16)\n",
      "Iter 2100 | Train loss 4.1463 (ppl 63.20) | Val loss 4.1890 (ppl 65.96)\n",
      "Iter 2200 | Train loss 4.1356 (ppl 62.53) | Val loss 4.1827 (ppl 65.54)\n",
      "Iter 2300 | Train loss 4.1403 (ppl 62.82) | Val loss 4.1716 (ppl 64.82)\n",
      "Iter 2400 | Train loss 4.1208 (ppl 61.61) | Val loss 4.1753 (ppl 65.06)\n",
      "Iter 2500 | Train loss 4.1265 (ppl 61.96) | Val loss 4.1584 (ppl 63.97)\n",
      "Iter 2600 | Train loss 4.1137 (ppl 61.17) | Val loss 4.1593 (ppl 64.03)\n",
      "Iter 2700 | Train loss 4.0939 (ppl 59.97) | Val loss 4.1541 (ppl 63.70)\n",
      "Iter 2800 | Train loss 4.1139 (ppl 61.18) | Val loss 4.1365 (ppl 62.58)\n",
      "Iter 2900 | Train loss 4.1031 (ppl 60.52) | Val loss 4.1529 (ppl 63.62)\n",
      "Iter 3000 | Train loss 4.0949 (ppl 60.03) | Val loss 4.1332 (ppl 62.38)\n",
      "Iter 3100 | Train loss 4.0713 (ppl 58.63) | Val loss 4.1266 (ppl 61.97)\n",
      "Iter 3200 | Train loss 4.0791 (ppl 59.09) | Val loss 4.1152 (ppl 61.26)\n",
      "Iter 3300 | Train loss 4.0930 (ppl 59.92) | Val loss 4.1281 (ppl 62.06)\n",
      "Iter 3400 | Train loss 4.0766 (ppl 58.95) | Val loss 4.0956 (ppl 60.08)\n",
      "Iter 3500 | Train loss 4.0737 (ppl 58.77) | Val loss 4.1224 (ppl 61.71)\n",
      "Iter 3600 | Train loss 4.0695 (ppl 58.53) | Val loss 4.1112 (ppl 61.02)\n",
      "Iter 3700 | Train loss 4.0762 (ppl 58.92) | Val loss 4.1281 (ppl 62.06)\n",
      "Iter 3800 | Train loss 4.0589 (ppl 57.91) | Val loss 4.1166 (ppl 61.35)\n",
      "Iter 3900 | Train loss 4.0527 (ppl 57.55) | Val loss 4.1063 (ppl 60.72)\n",
      "Iter 4000 | Train loss 4.0501 (ppl 57.40) | Val loss 4.0762 (ppl 58.92)\n",
      "Iter 4100 | Train loss 4.0637 (ppl 58.19) | Val loss 4.0904 (ppl 59.76)\n",
      "Iter 4200 | Train loss 4.0461 (ppl 57.18) | Val loss 4.0975 (ppl 60.19)\n",
      "Iter 4300 | Train loss 4.0457 (ppl 57.15) | Val loss 4.0954 (ppl 60.06)\n",
      "Iter 4400 | Train loss 4.0302 (ppl 56.27) | Val loss 4.0742 (ppl 58.81)\n",
      "Iter 4500 | Train loss 4.0291 (ppl 56.21) | Val loss 4.0772 (ppl 58.98)\n",
      "Iter 4600 | Train loss 4.0339 (ppl 56.48) | Val loss 4.0761 (ppl 58.91)\n",
      "Iter 4700 | Train loss 4.0224 (ppl 55.84) | Val loss 4.0749 (ppl 58.84)\n",
      "Iter 4800 | Train loss 4.0270 (ppl 56.09) | Val loss 4.0643 (ppl 58.23)\n",
      "Iter 4900 | Train loss 4.0313 (ppl 56.34) | Val loss 4.0592 (ppl 57.93)\n",
      "\n",
      "Level 3 complete:\n",
      "Final train loss: 4.0172\n",
      "Final val loss:   4.0661\n",
      "\n",
      "Sample:  Any, 2009, has broke place aftermath -- have luck the Talifi-Dofficent cities of beating the realistest victories of plans to benefits around the legal.\" One minister spokesmanerring to a friend . At least success of the intent to visit the vacative nation, equored 3,000 destroings flags and one hall fixance, and impression:© Frances New. HBo, My policys hours area. Turk Katas international nations carrying equipment, it's a mess earrest. Over she use the control of the propertes of his right and got him water, would do keep the cooper with the plants, we have sexually believe bulius' support for the airport census is like the rightabs and training -- is that. When World's 2013 confressed time, You would not complete each reduction officials in the Iraq . Teenus also decided assist his pill a mission -- which is watched into a prominution and park, the markle's aplace program was \"It's writual,\" said Southernion Taster. His most of Allen Balley, which he is effort to stop his statement that's about his Sali Kongs.\" Craiggental events between the debut and a catch Infordre to see-frisked hisildstitory. The trooks the tourat warm, particularly young via was bake as strangers, calories, to brextravel the book after the defining its white domest validary, celebrated illnesses. Israel's Tharebo Frm 14, is made it rightep. Anotherbravador: several children know, have had aware of that.\" Syrianhana convits as she left host his protesters of Civil Sultos \"-the second hundred on Monday,\" said Motions\" the\n",
      "Saved checkpoint: /Users/prakhar_patil/Desktop/SPE_Proj/TransformerMS/checkpoints/model_level3.pt\n",
      "\n",
      "=== LEVEL 4 TRAINING ===\n",
      "Iter    0 | Train loss 5.2988 (ppl 200.10) | Val loss 5.2941 (ppl 199.17)\n",
      "Iter  100 | Train loss 4.4550 (ppl 86.05) | Val loss 4.4525 (ppl 85.84)\n",
      "Iter  200 | Train loss 4.3035 (ppl 73.96) | Val loss 4.2986 (ppl 73.60)\n",
      "Iter  300 | Train loss 4.1921 (ppl 66.16) | Val loss 4.2032 (ppl 66.90)\n",
      "Iter  400 | Train loss 4.1378 (ppl 62.67) | Val loss 4.1516 (ppl 63.53)\n",
      "Iter  500 | Train loss 4.0908 (ppl 59.79) | Val loss 4.1047 (ppl 60.62)\n",
      "Iter  600 | Train loss 4.0788 (ppl 59.08) | Val loss 4.0746 (ppl 58.83)\n",
      "Iter  700 | Train loss 4.0374 (ppl 56.68) | Val loss 4.0642 (ppl 58.22)\n",
      "Iter  800 | Train loss 4.0267 (ppl 56.07) | Val loss 4.0275 (ppl 56.12)\n",
      "Iter  900 | Train loss 4.0119 (ppl 55.25) | Val loss 4.0039 (ppl 54.81)\n",
      "Iter 1000 | Train loss 3.9941 (ppl 54.28) | Val loss 4.0065 (ppl 54.95)\n",
      "Iter 1100 | Train loss 3.9913 (ppl 54.13) | Val loss 3.9869 (ppl 53.89)\n",
      "Iter 1200 | Train loss 3.9549 (ppl 52.19) | Val loss 3.9652 (ppl 52.73)\n",
      "Iter 1300 | Train loss 3.9432 (ppl 51.58) | Val loss 3.9569 (ppl 52.29)\n",
      "Iter 1400 | Train loss 3.9429 (ppl 51.57) | Val loss 3.9333 (ppl 51.08)\n",
      "Iter 1500 | Train loss 3.9604 (ppl 52.48) | Val loss 3.9321 (ppl 51.02)\n",
      "Iter 1600 | Train loss 3.9226 (ppl 50.53) | Val loss 3.9194 (ppl 50.37)\n",
      "Iter 1700 | Train loss 3.9057 (ppl 49.68) | Val loss 3.9090 (ppl 49.85)\n",
      "Iter 1800 | Train loss 3.9058 (ppl 49.69) | Val loss 3.9069 (ppl 49.75)\n",
      "Iter 1900 | Train loss 3.8998 (ppl 49.39) | Val loss 3.8997 (ppl 49.39)\n",
      "Iter 2000 | Train loss 3.8753 (ppl 48.20) | Val loss 3.8896 (ppl 48.89)\n",
      "Iter 2100 | Train loss 3.8710 (ppl 47.99) | Val loss 3.8732 (ppl 48.10)\n",
      "Iter 2200 | Train loss 3.8806 (ppl 48.45) | Val loss 3.8649 (ppl 47.70)\n",
      "Iter 2300 | Train loss 3.8839 (ppl 48.62) | Val loss 3.8690 (ppl 47.90)\n",
      "Iter 2400 | Train loss 3.8686 (ppl 47.87) | Val loss 3.8674 (ppl 47.82)\n",
      "Iter 2500 | Train loss 3.8649 (ppl 47.70) | Val loss 3.8788 (ppl 48.37)\n",
      "Iter 2600 | Train loss 3.8513 (ppl 47.06) | Val loss 3.8572 (ppl 47.33)\n",
      "Iter 2700 | Train loss 3.8468 (ppl 46.84) | Val loss 3.8612 (ppl 47.52)\n",
      "Iter 2800 | Train loss 3.8491 (ppl 46.95) | Val loss 3.8376 (ppl 46.41)\n",
      "Iter 2900 | Train loss 3.8103 (ppl 45.16) | Val loss 3.8434 (ppl 46.68)\n",
      "Iter 3000 | Train loss 3.8266 (ppl 45.91) | Val loss 3.8429 (ppl 46.66)\n",
      "Iter 3100 | Train loss 3.8247 (ppl 45.82) | Val loss 3.8246 (ppl 45.81)\n",
      "Iter 3200 | Train loss 3.8342 (ppl 46.26) | Val loss 3.8259 (ppl 45.87)\n",
      "Iter 3300 | Train loss 3.8285 (ppl 45.99) | Val loss 3.8150 (ppl 45.38)\n",
      "Iter 3400 | Train loss 3.8176 (ppl 45.49) | Val loss 3.8424 (ppl 46.64)\n",
      "Iter 3500 | Train loss 3.8140 (ppl 45.33) | Val loss 3.8231 (ppl 45.75)\n",
      "Iter 3600 | Train loss 3.8114 (ppl 45.21) | Val loss 3.8257 (ppl 45.86)\n",
      "Iter 3700 | Train loss 3.8197 (ppl 45.59) | Val loss 3.8096 (ppl 45.13)\n",
      "Iter 3800 | Train loss 3.7993 (ppl 44.67) | Val loss 3.7992 (ppl 44.67)\n",
      "Iter 3900 | Train loss 3.8025 (ppl 44.81) | Val loss 3.8072 (ppl 45.02)\n",
      "Iter 4000 | Train loss 3.7990 (ppl 44.65) | Val loss 3.7993 (ppl 44.67)\n",
      "Iter 4100 | Train loss 3.7847 (ppl 44.02) | Val loss 3.8059 (ppl 44.97)\n",
      "Iter 4200 | Train loss 3.7859 (ppl 44.08) | Val loss 3.8025 (ppl 44.81)\n",
      "Iter 4300 | Train loss 3.7872 (ppl 44.13) | Val loss 3.7946 (ppl 44.46)\n",
      "Iter 4400 | Train loss 3.7856 (ppl 44.06) | Val loss 3.8079 (ppl 45.05)\n",
      "Iter 4500 | Train loss 3.7837 (ppl 43.98) | Val loss 3.7737 (ppl 43.54)\n",
      "Iter 4600 | Train loss 3.7808 (ppl 43.85) | Val loss 3.7773 (ppl 43.70)\n",
      "Iter 4700 | Train loss 3.7696 (ppl 43.36) | Val loss 3.7764 (ppl 43.66)\n",
      "Iter 4800 | Train loss 3.7639 (ppl 43.11) | Val loss 3.7851 (ppl 44.04)\n",
      "Iter 4900 | Train loss 3.7600 (ppl 42.95) | Val loss 3.7929 (ppl 44.39)\n",
      "\n",
      "Level 4 complete:\n",
      "Final train loss: 3.7776\n",
      "Final val loss:   3.7958\n",
      "\n",
      "Sample: -level data (i.e., Goz Emity and the number of detection [1] located to observe that the condition of ValeN is much of the QPE toytimately prove utilizing the total of the kches within the encInmposing boxe, is a bey-evaryal N to also asidered ab constraints that conditions can beural to contain its simulation and are suprearities use for determinally that the weight connected set is association registering to predict their inigible patient network or semanticity that pseudo-ongive M. Game state for training. In the presence, proving the test sequence of rapid no correlation of the 3-classus formulation. But they on the development of their directions directions and a sok of the hidden t wireless biased environments (e.g., Vachines and Fadually), consider normangizing the form of the adversarial graph. Then, we define implemented rewards or satis. The fluation confirming performance is useful in. In the figure-Qymmptic variance, the data cjorgressivesitance a response. Ta Maprevane did Serve Section Iffice files. As described in Technical channels: Fellerine Scursivator-ciprovement. Note that: . getting t � denotical the time. (201_24)  Drigsels We randomly beformed by a solution where the cost value behind computing N and (erence valuation Mixture 3) have the section 2(_11 ), during i  1 Relinear track for registing channel, an example is insights the most relevant kind of region is revenue of visments. On our dataset, the hypothesisizes the test network. The following problem shown after hiddenies the 1\n",
      "Saved checkpoint: /Users/prakhar_patil/Desktop/SPE_Proj/TransformerMS/checkpoints/model_level4.pt\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:10:48.664879Z",
     "start_time": "2025-05-12T15:10:48.657724Z"
    }
   },
   "source": [
    "for i, level in enumerate(level_texts, start=1):\n",
    "    D = len(level)\n",
    "    total_tokens_seen = Config.block_size * Config.batch_size * Config.max_iters\n",
    "    epochs = total_tokens_seen / D\n",
    "    print(f\"Total Epochs for Level {i}: {epochs:.4f}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs for Level 1: 0.0245\n",
      "Total Epochs for Level 2: 0.0242\n",
      "Total Epochs for Level 3: 0.0244\n",
      "Total Epochs for Level 4: 0.0246\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:24:37.396345Z",
     "start_time": "2025-05-13T11:24:22.957585Z"
    }
   },
   "source": [
    "import sacrebleu\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load BART model/tokenizer once\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bart_model_name = 'facebook/bart-large-cnn'\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name).to(device)\n",
    "bart_model.eval()\n",
    "\n",
    "# Function to calculate BLEU\n",
    "def compute_bleu(references, hypotheses):\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
    "    return bleu.score\n",
    "\n",
    "# Function to calculate BARTScore\n",
    "def compute_bart_score(references, hypotheses):\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        input_ids = bart_tokenizer(hyp, return_tensors=\"pt\").input_ids.to(device)\n",
    "        with bart_tokenizer.as_target_tokenizer():\n",
    "            target_ids = bart_tokenizer(ref, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = bart_model(input_ids=input_ids, labels=target_ids)\n",
    "            log_likelihood = -output.loss * target_ids.size(1)\n",
    "            scores.append(log_likelihood.item())\n",
    "    \n",
    "    return sum(scores) / len(scores)\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T15:12:17.046435Z",
     "start_time": "2025-05-12T15:12:16.817270Z"
    }
   },
   "source": [
    "# Generate a sample from your model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=Config.device)\n",
    "sample_ids = model.generate(context, max_new_tokens=50)[0].tolist()\n",
    "generated_text = tokenizer.decode(sample_ids)\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# encoded = tokenizer.encode(full_text).ids\n",
    "# print(f\"Encoding took {time.time() - start:.2f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:54:49.497501Z",
     "start_time": "2025-05-13T11:54:23.298814Z"
    }
   },
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Step 1: Take last 10% of the raw text\n",
    "start = time.time()\n",
    "val_text = full_text[int(0.9 * len(full_text)):]  # slicing the string\n",
    "print(f\"Text slicing took {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Step 2: Tokenize only the validation text\n",
    "start = time.time()\n",
    "val_ids = tokenizer.encode(val_text).ids\n",
    "print(f\"Tokenizing val_text took {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Step 3: Convert to tensor and move to device\n",
    "start = time.time()\n",
    "val_data = torch.tensor(val_ids, dtype=torch.long)\n",
    "print(f\"Tensor creation and device transfer took {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Step 4: Sample first 50 tokens and decode\n",
    "start = time.time()\n",
    "ref_ids = val_data[:50].tolist()\n",
    "reference_text = tokenizer.decode(ref_ids)\n",
    "print(f\"Decoding took {time.time() - start:.2f} seconds\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text slicing took 0.26 seconds\n",
      "Tokenizing val_text took 25.34 seconds\n",
      "Tensor creation and device transfer took 0.56 seconds\n",
      "Decoding took 0.01 seconds\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reference and generated texts (can be lists from your evaluation step)\n",
    "# references = [\"The cat is sitting on the mat.\", \"The weather is sunny today.\"]\n",
    "# generated = [\"A cat is on the mat.\", \"It is sunny outside.\"]\n",
    "references = [reference_text]\n",
    "generated = [generated_text]\n",
    "\n",
    "\n",
    "bleu = compute_bleu(references, generated)\n",
    "bart = compute_bart_score(references, generated)\n",
    "\n",
    "print(f\"\\nBLEU Score:     {bleu:.2f}\")\n",
    "print(f\"BARTScore (avg): {bart:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# # ------------------ Hyperparameters ------------------\n",
    "# batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "# block_size = 32 # what is the maximum context length for predictions?\n",
    "# max_iters = 5000\n",
    "# eval_interval = 100\n",
    "# learning_rate = 1e-3\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embd = 64\n",
    "# n_head = 4\n",
    "# n_layer = 4\n",
    "# dropout = 0.0\n",
    "# # # ------------\n",
    "# # # # hyperparameters for GPU\n",
    "# # batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "# # block_size = 512 # what is the maximum context length for predictions?\n",
    "# # max_iters = 10000\n",
    "# # eval_interval = 500\n",
    "# # learning_rate = 3e-4\n",
    "# # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # eval_iters = 200\n",
    "# # n_embd = 384\n",
    "# # n_head = 6\n",
    "# # n_layer = 6\n",
    "# # dropout = 0.2\n",
    "# # # # ------------\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "# # ------------------ BPE tokenizer functions (assumed defined) ------------------\n",
    "# # encode(text) -> list of token ids\n",
    "# # decode(ids)   -> string\n",
    "# # merges, vocab, etc. already built above in your notebook\n",
    "\n",
    "# # ------------------ Data loader for transformer ------------------\n",
    "# def get_batch_transformer(data):\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     x  = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "#     y  = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "#     return x, y\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss_transformer(model, train_data, val_data):\n",
    "#     model.eval()\n",
    "#     out = {}\n",
    "#     for split, data in zip(['train','val'], [train_data, val_data]):\n",
    "#         losses = []\n",
    "#         for _ in range(eval_iters):\n",
    "#             X, Y = get_batch_transformer(data)\n",
    "#             _, loss = model(X, Y)\n",
    "#             losses.append(loss.item())\n",
    "#         out[split] = sum(losses) / len(losses)\n",
    "#     model.train()\n",
    "#     return out\n",
    "\n",
    "# def compute_perplexity(loss):\n",
    "#     return torch.exp(torch.tensor(loss))\n",
    "\n",
    "# # ------------------ Transformer model classes ------------------\n",
    "# class Head(nn.Module):\n",
    "#     def __init__(self, head_size):\n",
    "#         super().__init__()\n",
    "#         self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "#         self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "#         self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "#         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, x):\n",
    "#         B, T, C = x.shape\n",
    "#         k = self.key(x); q = self.query(x)\n",
    "#         wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "#         wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "#         wei = F.softmax(wei, dim=-1)\n",
    "#         wei = self.dropout(wei)\n",
    "#         v = self.value(x)\n",
    "#         return wei @ v\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, num_heads, head_size):\n",
    "#         super().__init__()\n",
    "#         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "#         self.proj  = nn.Linear(n_embd, n_embd)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, x):\n",
    "#         out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "#         return self.dropout(self.proj(out))\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, n_embd):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(n_embd, 4*n_embd),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4*n_embd, n_embd),\n",
    "#             nn.Dropout(dropout),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, n_embd, n_head):\n",
    "#         super().__init__()\n",
    "#         head_size = n_embd // n_head\n",
    "#         self.sa    = MultiHeadAttention(n_head, head_size)\n",
    "#         self.ffwd  = FeedForward(n_embd)\n",
    "#         self.ln1   = nn.LayerNorm(n_embd)\n",
    "#         self.ln2   = nn.LayerNorm(n_embd)\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.sa(self.ln1(x))   # Residual connection after self-attention\n",
    "#         x = x + self.ffwd(self.ln2(x)) # Residual connection after feedforward\n",
    "#         return x\n",
    "\n",
    "# class GPTLanguageModel(nn.Module):\n",
    "#     def __init__(self, vocab_size):\n",
    "#         super().__init__()\n",
    "#         self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
    "#         self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "#         self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "#         self.ln_f   = nn.LayerNorm(n_embd)\n",
    "#         self.lm_head= nn.Linear(n_embd, vocab_size)\n",
    "#         self.apply(self._init_weights)\n",
    "#     def _init_weights(self, module):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "#             if module.bias is not None:\n",
    "#                 nn.init.zeros_(module.bias)\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "#     def forward(self, idx, targets=None):\n",
    "#         B, T = idx.shape\n",
    "#         tok_emb = self.token_embedding_table(idx)\n",
    "#         pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "#         x = tok_emb + pos_emb\n",
    "#         x = self.blocks(x)\n",
    "#         x = self.ln_f(x)\n",
    "#         logits = self.lm_head(x)\n",
    "#         loss = None\n",
    "#         if targets is not None:\n",
    "#             B, T, C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             targets= targets.view(B*T)\n",
    "#             loss   = F.cross_entropy(logits, targets)\n",
    "#         return logits, loss\n",
    "#     def generate(self, idx, max_new_tokens):\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             idx_cond = idx[:, -block_size:]\n",
    "#             logits, _= self(idx_cond)\n",
    "#             logits = logits[:, -1, :]\n",
    "#             probs  = F.softmax(logits, dim=-1)\n",
    "#             idx_next = torch.multinomial(probs, num_samples=1)\n",
    "#             idx = torch.cat((idx, idx_next), dim=1)\n",
    "#         return idx\n",
    "\n",
    "# # ------------------ Incremental Training Pipeline ------------------\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def incremental_training_pipeline(level_texts, model, checkpoint_dir=\"checkpoints\"):\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "#     # Store metrics for plotting\n",
    "#     all_metrics = {\n",
    "#         \"level\": [],\n",
    "#         \"iters\": [],\n",
    "#         \"train_loss\": [],\n",
    "#         \"val_loss\": [],\n",
    "#         \"train_ppl\": [],\n",
    "#         \"val_ppl\": [],\n",
    "#     }\n",
    "\n",
    "#     for level_i, level_text in enumerate(level_texts, start=1):\n",
    "#         print(f\"\\n=== LEVEL {level_i} TRAINING ===\")\n",
    "#         data_ids   = torch.tensor(encode(level_text), dtype=torch.long).to(device)\n",
    "#         n          = int(0.9 * data_ids.size(0))\n",
    "#         train_data = data_ids[:n]\n",
    "#         val_data   = data_ids[n:]\n",
    "\n",
    "#         optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#         for it in range(max_iters):\n",
    "#             if it % eval_interval == 0:\n",
    "#                 losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "#                 ppl_train = compute_perplexity(losses['train'])\n",
    "#                 ppl_val   = compute_perplexity(losses['val'])\n",
    "\n",
    "#                 # Store for later plotting\n",
    "#                 all_metrics[\"level\"].append(level_i)\n",
    "#                 all_metrics[\"iters\"].append(it)\n",
    "#                 all_metrics[\"train_loss\"].append(losses['train'])\n",
    "#                 all_metrics[\"val_loss\"].append(losses['val'])\n",
    "#                 all_metrics[\"train_ppl\"].append(ppl_train.item())\n",
    "#                 all_metrics[\"val_ppl\"].append(ppl_val.item())\n",
    "\n",
    "#                 print(f\" it={it:4d} | train loss {losses['train']:.4f}  ppl {ppl_train:.2f}  | \"\n",
    "#                       f\"val loss {losses['val']:.4f}  ppl {ppl_val:.2f}\")\n",
    "\n",
    "#             xb, yb = get_batch_transformer(train_data)\n",
    "#             logits, loss = model(xb, yb)\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # final evaluation\n",
    "#         losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "#         ppl_train = compute_perplexity(losses['train'])\n",
    "#         ppl_val   = compute_perplexity(losses['val'])\n",
    "\n",
    "#         print(f\"\\n*** Level {level_i} complete.\")\n",
    "#         print(f\"    Final train loss {losses['train']:.4f}, ppl {ppl_train:.2f}\")\n",
    "#         print(f\"    Final   val loss {losses['val']:.4f}, ppl {ppl_val:.2f}\")\n",
    "\n",
    "#         # sample generation\n",
    "#         context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "#         sample_ids = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "#         print(\"    Sample:\", decode(sample_ids))\n",
    "\n",
    "#         # save checkpoint\n",
    "#         cp = os.path.join(checkpoint_dir, f\"model_level{level_i}.pt\")\n",
    "#         torch.save(model.state_dict(), cp)\n",
    "#         print(f\"    → Saved checkpoint: {cp}\")\n",
    "\n",
    "#     return model, all_metrics\n",
    "\n",
    "\n",
    "# # ------------------ Usage ------------------\n",
    "# # Make sure: L1, L2, L3, L4_cleaned are already in your notebook\n",
    "# level_texts = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]\n",
    "\n",
    "# # 'vocab_size' must match your BPE final vocabulary size (e.g. 276)\n",
    "# vocab_size = tokenizer.get_vocab_size()\n",
    "# model = GPTLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# model,all_metrics = incremental_training_pipeline(level_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, level in enumerate(level_texts, start=1):\n",
    "    D = len(level)\n",
    "    total_tokens_seen = Config.block_size * Config.batch_size * Config.max_iters\n",
    "    epochs = total_tokens_seen / D\n",
    "    print(f\"Total Epochs for Level {i}: {epochs:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Shakespeare input prompt\n",
    "# # # Example Shakespearean prompt\n",
    "# prompt = \"Who is the President of USA \"\n",
    "\n",
    "# # # Encode the prompt using your trained BPE tokenizer\n",
    "# encoded = encode(prompt)\n",
    "\n",
    "# # # Truncate if the prompt is longer than block_size\n",
    "# if len(encoded) > block_size:\n",
    "#     encoded = encoded[-block_size:]\n",
    "\n",
    "# # # Create context tensor\n",
    "# context = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "\n",
    "# # # Generate continuation\n",
    "# generated_ids = model.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "\n",
    "# # # Decode and print the generated text\n",
    "# print(decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(all_metrics):\n",
    "    levels = sorted(set(all_metrics[\"level\"]))\n",
    "    for level in levels:\n",
    "        # Get indices for this level\n",
    "        idxs = [i for i, l in enumerate(all_metrics[\"level\"]) if l == level]\n",
    "        iters = [all_metrics[\"iters\"][i] for i in idxs]\n",
    "        val_loss = [all_metrics[\"val_loss\"][i] for i in idxs]\n",
    "        val_ppl  = [all_metrics[\"val_ppl\"][i] for i in idxs]\n",
    "\n",
    "        # Create figure and first y-axis (for loss)\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.plot(iters, val_loss, color='blue', label='Validation Loss')\n",
    "        ax1.set_xlabel('Iterations')\n",
    "        ax1.set_ylabel('Validation Loss', color='blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Second y-axis for perplexity\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(iters, val_ppl, color='green', label='Validation Perplexity')\n",
    "        ax2.set_ylabel('Validation Perplexity', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "        plt.title(f'Level {level} - Validation Loss & Perplexity')\n",
    "        fig.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_metrics(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(model, '../Joblibs/transformer.joblib')\n",
    "joblib.dump(model, os.path.join(BASE_DIR, 'Joblibs', 'transformer.joblib'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'block_size': Config.block_size,\n",
    "        'n_embd': Config.n_embd,\n",
    "        'n_head': Config.n_head,\n",
    "        'n_layer': Config.n_layer,\n",
    "        'dropout': Config.dropout\n",
    "    }\n",
    "}, os.path.join(BASE_DIR, 'Joblibs', 'gpt_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tokenizer, os.path.join(BASE_DIR, 'Joblibs', 'tokenizer.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7165480,
     "sourceId": 11438992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212951,
     "sourceId": 11504366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7213450,
     "sourceId": 11504999,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
