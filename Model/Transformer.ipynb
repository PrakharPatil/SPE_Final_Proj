{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:24:04.941069Z",
     "start_time": "2025-04-25T09:24:04.933057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  # 👈 force reload if values were already set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 root MySQL31# text_dataset_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv('DB_HOST'), os.getenv('DB_USER'), os.getenv('DB_PASSWORD'), os.getenv('DB_NAME'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Data Sample:  Once upon a time in the land of Policymia, there lived two leaders named Majora and Minoro. Their job was to make sure all the citizens had beautiful parks, clean water, and top-notch schools. But there were so many things to fix! How would they ever decide where to start?\n",
      "L2 Data Sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "L3 Data Sample: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n",
      "L4 Data Sample: Research on Neural Machine Translation Model\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# ✅ Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Fetch database configuration from environment variables\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'database': os.getenv('DB_NAME')\n",
    "}\n",
    "\n",
    "# ✅ Connect to MySQL using PyMySQL\n",
    "conn = pymysql.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ✅ Define a function to fetch content based on the level\n",
    "def fetch_data_by_level(level):\n",
    "    query = \"SELECT content FROM text_data WHERE level = %s\"\n",
    "    cursor.execute(query, (level,))\n",
    "    rows = cursor.fetchall()\n",
    "    return [row[0] for row in rows]\n",
    "\n",
    "# ✅ Fetch data for each level\n",
    "L1= fetch_data_by_level('L1')\n",
    "L2 = fetch_data_by_level('L2')\n",
    "L3 = fetch_data_by_level('L3')\n",
    "L4 = fetch_data_by_level('L4')\n",
    "\n",
    "# ✅ Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# ✅ Print samples (optional)\n",
    "print(\"L1 Data Sample:\", L1[0][:5000] if L1 else \"No data found\")\n",
    "print(\"L2 Data Sample:\", L2[0][:500] if L2 else \"No data found\")\n",
    "print(\"L3 Data Sample:\", L3[0][:500] if L3 else \"No data found\")\n",
    "print(\"L4 Data Sample:\", L4[0][:5000] if L4 else \"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all content from L1, L2, L3, L4 into single strings\n",
    "L1_combined = \" \".join(L1) if L1 else \"\"\n",
    "L2_combined = \" \".join(L2) if L2 else \"\"\n",
    "L3_combined = \" \".join(L3) if L3 else \"\"\n",
    "L4_combined = \" \".join(L4) if L4 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Data Sample:  Once upon a time in the land of Policymia, there lived two leaders named Majora and Minoro. Their job was to make sure all the citizens had beautiful parks, clean water, and top-notch schools. But there were so many things to fix! How would they ever decide where to start?  Majora, being the wise leader she was, knew just what to do. She invited her fellow policymakers for a big meeting at the Roundtable of Representatives. There, they discussed the most important problems Policymia faced. This\n",
      "L2 Data Sample: usually , he would be tearing around the living room , playing with his toys . but just one look at a minion sent him practically catatonic . that had been megan 's plan when she got him dressed earlier . he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older . she liked to think being surrounded by adults and older kids was one reason why he was a such a \n",
      "L3 Data Sample: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n",
      "L4 Data Sample: Research on Neural Machine Translation Model  In neural machine translation (NMT), cyclic neural networks, especially long-term and short-term memory networks and gated recurrent neural networks, have been regarded as the latest methods for sequence modeling and transduction problems for a long time, such as language modeling and machine translation. When the cyclic neural network is running, the sequence information is processed one by one, strictly following the order from left to right or fro\n"
     ]
    }
   ],
   "source": [
    "print(\"L1 Data Sample:\", L1_combined[:500] )\n",
    "print(\"L2 Data Sample:\", L2_combined[:500] )\n",
    "print(\"L3 Data Sample:\", L3_combined[:500] )\n",
    "print(\"L4 Data Sample:\", L4_combined[:500] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:39:26.053959Z",
     "start_time": "2025-04-24T14:39:24.227751Z"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# with open(\"Data/L1_ChildrenStories.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L1 = f.read()\n",
    "# with open(\"Data/L2_BookCorpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L2 = f.read()\n",
    "# with open(\"Data/L3_CNN_DailyMail.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L3 = f.read()\n",
    "# with open(\"Data/L4_S2ORC.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L4 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:39:30.985039Z",
     "start_time": "2025-04-24T14:39:29.994292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Define allowed character sets\n",
    "english_regex = r\"[a-zA-Z0-9\\s]\"                   # English letters, numbers, spaces\n",
    "math_symbols  = r\"[\\+\\-\\*/=<>∑∫√πθΣ∂∞]\"             # Add more math symbols if needed\n",
    "special_chars = r\"[\\.,!?;:'\\\"()\\[\\]{}#@%^&*_~]\"     # Common special characters\n",
    "\n",
    "# Function to clean a given text\n",
    "def clean_text(text):\n",
    "    return \"\".join(\n",
    "        c for c in text\n",
    "        if re.match(english_regex, c) or \n",
    "           re.match(math_symbols, c) or \n",
    "           re.match(special_chars, c) or \n",
    "           emoji.is_emoji(c)\n",
    "    )\n",
    "\n",
    "# Apply cleaning to all levels\n",
    "# L1_cleaned = clean_text(L1)\n",
    "# L2_cleaned = clean_text(L2)\n",
    "# L3_cleaned = clean_text(L3)\n",
    "# L4_cleaned = clean_text(L4)\n",
    "L1_cleaned = clean_text(L1_combined)\n",
    "L2_cleaned = clean_text(L2_combined)\n",
    "L3_cleaned = clean_text(L3_combined)\n",
    "L4_cleaned = clean_text(L4_combined)\n",
    "print(\"All levels cleaned and stored in *_cleaned variables!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = L1_cleaned+L2_cleaned+L3_cleaned+L4_cleaned\n",
    "D = len(full_text)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "token = full_text.encode(\"utf-8\")\n",
    "def get_stats(ids):\n",
    "    counts = {} # Creates an empty Dictionary\n",
    "    for pair in zip(ids,ids[1:]):\n",
    "        counts[pair] = counts.get(pair ,0)+1\n",
    "    return counts\n",
    "print(vocab_size)\n",
    "print(''.join(chars))\n",
    "stats = get_stats(token)\n",
    "top_pair = max(stats,key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "\n",
    "# Your cleaned text data\n",
    "docs = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]\n",
    "\n",
    "# Break large documents into smaller lines/chunks\n",
    "def chunked_docs():\n",
    "    for doc in docs:\n",
    "        # You can tweak the split here (e.g., '. ' or '\\n' or custom logic)\n",
    "        for line in doc.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield line\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "# Trainer with manageable vocab size\n",
    "trainer = BpeTrainer(vocab_size=2000, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "# Train using iterator to save memory\n",
    "tokenizer.train_from_iterator(chunked_docs(), trainer=trainer)\n",
    "\n",
    "# Check final vocab size\n",
    "print(\"Actual vocab size:\", tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(token_ids: list[int]) -> str:\n",
    "    return tokenizer.decode(token_ids)\n",
    "sample = \"math is beautiful ✨\"\n",
    "ids = encode(sample)\n",
    "print(\"→\", ids)\n",
    "print(\"←\", decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Like Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# ------------------ Hyperparameters ------------------\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# # ------------\n",
    "# # # hyperparameters for GPU\n",
    "# batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "# block_size = 512 # what is the maximum context length for predictions?\n",
    "# max_iters = 10000\n",
    "# eval_interval = 500\n",
    "# learning_rate = 3e-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embd = 384\n",
    "# n_head = 6\n",
    "# n_layer = 6\n",
    "# dropout = 0.2\n",
    "# # # ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# ------------------ BPE tokenizer functions (assumed defined) ------------------\n",
    "# encode(text) -> list of token ids\n",
    "# decode(ids)   -> string\n",
    "# merges, vocab, etc. already built above in your notebook\n",
    "\n",
    "# ------------------ Data loader for transformer ------------------\n",
    "def get_batch_transformer(data):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x  = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y  = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_transformer(model, train_data, val_data):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, data in zip(['train','val'], [train_data, val_data]):\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            X, Y = get_batch_transformer(data)\n",
    "            _, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))\n",
    "\n",
    "# ------------------ Transformer model classes ------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x); q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa    = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd  = FeedForward(n_embd)\n",
    "        self.ln1   = nn.LayerNorm(n_embd)\n",
    "        self.ln2   = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))   # Residual connection after self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # Residual connection after feedforward\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f   = nn.LayerNorm(n_embd)\n",
    "        self.lm_head= nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets= targets.view(B*T)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _= self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs  = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# ------------------ Incremental Training Pipeline ------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def incremental_training_pipeline(level_texts, model, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    all_metrics = {\n",
    "        \"level\": [],\n",
    "        \"iters\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_ppl\": [],\n",
    "        \"val_ppl\": [],\n",
    "    }\n",
    "\n",
    "    for level_i, level_text in enumerate(level_texts, start=1):\n",
    "        print(f\"\\n=== LEVEL {level_i} TRAINING ===\")\n",
    "        data_ids   = torch.tensor(encode(level_text), dtype=torch.long).to(device)\n",
    "        n          = int(0.9 * data_ids.size(0))\n",
    "        train_data = data_ids[:n]\n",
    "        val_data   = data_ids[n:]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        for it in range(max_iters):\n",
    "            if it % eval_interval == 0:\n",
    "                losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "                ppl_train = compute_perplexity(losses['train'])\n",
    "                ppl_val   = compute_perplexity(losses['val'])\n",
    "\n",
    "                # Store for later plotting\n",
    "                all_metrics[\"level\"].append(level_i)\n",
    "                all_metrics[\"iters\"].append(it)\n",
    "                all_metrics[\"train_loss\"].append(losses['train'])\n",
    "                all_metrics[\"val_loss\"].append(losses['val'])\n",
    "                all_metrics[\"train_ppl\"].append(ppl_train.item())\n",
    "                all_metrics[\"val_ppl\"].append(ppl_val.item())\n",
    "\n",
    "                print(f\" it={it:4d} | train loss {losses['train']:.4f}  ppl {ppl_train:.2f}  | \"\n",
    "                      f\"val loss {losses['val']:.4f}  ppl {ppl_val:.2f}\")\n",
    "\n",
    "            xb, yb = get_batch_transformer(train_data)\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # final evaluation\n",
    "        losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "        ppl_train = compute_perplexity(losses['train'])\n",
    "        ppl_val   = compute_perplexity(losses['val'])\n",
    "\n",
    "        print(f\"\\n*** Level {level_i} complete.\")\n",
    "        print(f\"    Final train loss {losses['train']:.4f}, ppl {ppl_train:.2f}\")\n",
    "        print(f\"    Final   val loss {losses['val']:.4f}, ppl {ppl_val:.2f}\")\n",
    "\n",
    "        # sample generation\n",
    "        context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "        sample_ids = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "        print(\"    Sample:\", decode(sample_ids))\n",
    "\n",
    "        # save checkpoint\n",
    "        cp = os.path.join(checkpoint_dir, f\"model_level{level_i}.pt\")\n",
    "        torch.save(model.state_dict(), cp)\n",
    "        print(f\"    → Saved checkpoint: {cp}\")\n",
    "\n",
    "    return model, all_metrics\n",
    "\n",
    "\n",
    "# ------------------ Usage ------------------\n",
    "# Make sure: L1, L2, L3, L4_cleaned are already in your notebook\n",
    "level_texts = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]\n",
    "\n",
    "# 'vocab_size' must match your BPE final vocabulary size (e.g. 276)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "\n",
    "model,all_metrics = incremental_training_pipeline(level_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, level in enumerate(level_texts, start=1):\n",
    "    D = len(level)\n",
    "    total_tokens_seen = block_size * batch_size * max_iters\n",
    "    epochs = total_tokens_seen / D\n",
    "    print(f\"Total Epochs for Level {i}: {epochs:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Shakespeare input prompt\n",
    "# # # Example Shakespearean prompt\n",
    "# prompt = \"Who is the President of USA \"\n",
    "#\n",
    "# # # Encode the prompt using your trained BPE tokenizer\n",
    "# encoded = encode(prompt)\n",
    "#\n",
    "# # # Truncate if the prompt is longer than block_size\n",
    "# if len(encoded) > block_size:\n",
    "#     encoded = encoded[-block_size:]\n",
    "#\n",
    "# # # Create context tensor\n",
    "# context = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "#\n",
    "# # # Generate continuation\n",
    "# generated_ids = model.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "#\n",
    "# # # Decode and print the generated text\n",
    "# print(decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(all_metrics):\n",
    "    levels = sorted(set(all_metrics[\"level\"]))\n",
    "    for level in levels:\n",
    "        # Get indices for this level\n",
    "        idxs = [i for i, l in enumerate(all_metrics[\"level\"]) if l == level]\n",
    "        iters = [all_metrics[\"iters\"][i] for i in idxs]\n",
    "        val_loss = [all_metrics[\"val_loss\"][i] for i in idxs]\n",
    "        val_ppl  = [all_metrics[\"val_ppl\"][i] for i in idxs]\n",
    "\n",
    "        # Create figure and first y-axis (for loss)\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.plot(iters, val_loss, color='blue', label='Validation Loss')\n",
    "        ax1.set_xlabel('Iterations')\n",
    "        ax1.set_ylabel('Validation Loss', color='blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Second y-axis for perplexity\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(iters, val_ppl, color='green', label='Validation Perplexity')\n",
    "        ax2.set_ylabel('Validation Perplexity', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "        plt.title(f'Level {level} - Validation Loss & Perplexity')\n",
    "        fig.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'transformer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7165480,
     "sourceId": 11438992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212951,
     "sourceId": 11504366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7213450,
     "sourceId": 11504999,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
