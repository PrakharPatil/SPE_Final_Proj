{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:24:04.941069Z",
     "start_time": "2025-04-25T09:24:04.933057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  # 👈 force reload if values were already set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 root MySQL31# text_dataset_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv('DB_HOST'), os.getenv('DB_USER'), os.getenv('DB_PASSWORD'), os.getenv('DB_NAME'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Data Sample:  Once upon a time in the land of Policymia, there lived two leaders named Majora and Minoro. Their job was to make sure all the citizens had beautiful parks, clean water, and top-notch schools. But there were so many things to fix! How would they ever decide where to start?\n",
      "L2 Data Sample: usually , he would be tearing around the living room , playing with his toys .\n",
      "L3 Data Sample: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n",
      "L4 Data Sample: Research on Neural Machine Translation Model\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# ✅ Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Fetch database configuration from environment variables\n",
    "db_config = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'database': os.getenv('DB_NAME')\n",
    "}\n",
    "\n",
    "# ✅ Connect to MySQL using PyMySQL\n",
    "conn = pymysql.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ✅ Define a function to fetch content based on the level\n",
    "def fetch_data_by_level(level):\n",
    "    query = \"SELECT content FROM text_data WHERE level = %s\"\n",
    "    cursor.execute(query, (level,))\n",
    "    rows = cursor.fetchall()\n",
    "    return [row[0] for row in rows]\n",
    "\n",
    "# ✅ Fetch data for each level\n",
    "L1= fetch_data_by_level('L1')\n",
    "L2 = fetch_data_by_level('L2')\n",
    "L3 = fetch_data_by_level('L3')\n",
    "L4 = fetch_data_by_level('L4')\n",
    "\n",
    "# ✅ Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# ✅ Print samples (optional)\n",
    "print(\"L1 Data Sample:\", L1[0][:5000] if L1 else \"No data found\")\n",
    "print(\"L2 Data Sample:\", L2[0][:500] if L2 else \"No data found\")\n",
    "print(\"L3 Data Sample:\", L3[0][:500] if L3 else \"No data found\")\n",
    "print(\"L4 Data Sample:\", L4[0][:5000] if L4 else \"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all content from L1, L2, L3, L4 into single strings\n",
    "L1_combined = \" \".join(L1) if L1 else \"\"\n",
    "L2_combined = \" \".join(L2) if L2 else \"\"\n",
    "L3_combined = \" \".join(L3) if L3 else \"\"\n",
    "L4_combined = \" \".join(L4) if L4 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Data Sample:  Once upon a time in the land of Policymia, there lived two leaders named Majora and Minoro. Their job was to make sure all the citizens had beautiful parks, clean water, and top-notch schools. But there were so many things to fix! How would they ever decide where to start?  Majora, being the wise leader she was, knew just what to do. She invited her fellow policymakers for a big meeting at the Roundtable of Representatives. There, they discussed the most important problems Policymia faced. This was called identifying \"key policy areas.\" It meant figuring out which topics needed attention first.  Next came assessing support – finding out if everyone agreed on the solutions. Some people thought building more playgrounds was the way to go, while others wanted better libraries. To understand everyone's thoughts, Majora used something called 'polling.' Just like taking a vote, polling helped her see what ideas were popular among her friends (the majority) and also those across the aisle (people who didn't belong to her political group).  While talking to her friends and colleagues, Majora discovered that almost everyone loved science! And why not? Science could help create amazing inventions, protect nature, and even cure sicknesses. So, the policymakers decided to build a super cool SCIENCE CENTER right at the heart of Policymia!  But then, an unexpected problem popped up! A grumpy neighboring kingdom threatened to block Policymia's plans because they feared losing visitors to the new center. Oh no! However, instead of giving up, Majora saw this challenge as an opportunity. If they could work together with the grumpy neighbors, maybe both lands could benefit. That way, everybody wins, showing the true spirit of teamwork and collaboration in the world of policies!  In the bustling city of Brooklyn, there was a special movie screening for a new film called \"Cross Eyed.\" Billy and his friends were so excited to see it because they heard it was full of laughter and fun.  As soon as the movie started, they met peculiar characters who brought nonstop giggles. From talking animals to humans with unusual talents, each character had their own quirky charm. Every role, even those on the sidelines, felt important and added something special to the mix.  Billy's favorite part of the movie involved little inventions called \"gadgets,\" created by a mad scientist named Professor Zoom. These gizmos popped up unexpectedly during scenes, adding layers of humor and excitement. With every appearance, the professor explained how these devices worked, teaching everyone about fascinating scientific principles.  During recess, Billy couldn't stop thinking about the movie. He shared all he learned with his classmates, describing the gadgets and what they did. Together, they imagined creating their own silly contraptions while discussing forces, energy, and motion.  Unfortunately, after hearing about the fantastic movie, none of Billy's friends could join him for a second viewing. Though disappointed, he realized that sharing knowledge with others can spread joy far beyond himself. Sometimes our discoveries don't turn out exactly as we hope, but learning valuable lessons along the way makes us grow stronger in both mind and spirit.  Once upon a time, in a small town named Harmonyville, lived two best friends, Benny the Brainy Beaver and Sally the Social Squirrel. They were both excited because they had applied for jobs at the local hospital, hoping to become healthcare helpers. Even though they didn't have any personal experience working in healthcare, they wanted to be prepared for their interviews. So, they decided to seek advice from Dr. Wise Owl, the wisest creature in Harmonyville.  Dr. Wise Owl greeted them warmly and said, \"To prepare for your interviews, first, do your research! Learn about Healing Hollow Hospital and its mission statement.\" Benny and Sally nodded, curious about what they might find out. They discovered that Healing Hollow was dedicated to providing excellent care while promoting wellness and disease prevention. This made them even more eager to contribute to such an important cause.  Next, Dr. Wise Owl suggested, \"Reviewing your resumes is essential. Make sure you can discuss every skill, experience, and qualification mentioned in detail.\" As they looked over their resumes together, Benny realized he needed to brush up on his understanding of human anatomy. With Sally's encouragement, they visited the Harmonyville library, where they found books filled with fascinating information about bones, muscles, and organs. Learning these scientific facts would not only impress their future employers but also benefit their patients one day.  As their preparations continued, Dr. Wise Owl shared another tip, \"Understand the current trends and latest advancements in the healthcare industry!\" Curious by nature, Benny and Sally couldn't wait to dive into learning about new treatments, medical devices, and regulatory updates. \n",
      "L2 Data Sample: usually , he would be tearing around the living room , playing with his toys . but just one look at a minion sent him practically catatonic . that had been megan 's plan when she got him dressed earlier . he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older . she liked to think being surrounded by adults and older kids was one reason why he was a such a \n",
      "L3 Data Sample: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n",
      "L4 Data Sample: Research on Neural Machine Translation Model  In neural machine translation (NMT), cyclic neural networks, especially long-term and short-term memory networks and gated recurrent neural networks, have been regarded as the latest methods for sequence modeling and transduction problems for a long time, such as language modeling and machine translation. When the cyclic neural network is running, the sequence information is processed one by one, strictly following the order from left to right or from right to left, processing one word at a time, and parallel operation cannot be realized, resulting in slow running speed. With the rapid development of neural machine translation (NMT) network architecture, cyclic neural network has been effectively replaced by convolution network and self - attention. Convolution neural network has replaced the divine circulation neural network due to its parallel computation of convolution. The Transformer model replaces the long-term and short-term memory network with a complete self-attention structure, and abandons the traditional encoder and decoder model which must combine the inherent mode of convolutional neural network or circular neural network and only uses the self-attention mechanism. Although the biggest innovation of Transformer architecture is to use full self - attention, there are several other factors, such as multi-head attention and residual connection. The model flexibly combine several common building blocks in the Transformer architecture with the cyclic neural network. By borrowing the framework of the Transformer architecture without using full self - attention, experiments show that the cyclic model can be very close to the performance of the Transformer Our model achieved 26.7 BLEU in the WMT 2014 English to German translation task and 37.8 BLEU in the WMT 2014 English to French translation task. Using these two scores alone is very close to the score of the Transformer architecture using full attention, so even if the cyclic neural network is used instead of full self - attention, it can perform well on the data set.   Introduction Machine translation is that a computer can automatically translate one human language into another human language, thus realizing automatic conversion between different languages [1]. Traditional machine translation methods include rule-based machine translation, instance-based machine translation and statistics-based machine translation. With the rapid development of artificial intelligence, depth-based learning has also been widely used in the field of natural language processing. The emergence of neural machine translation (NMT) has significantly improved the quality of machine translation, thus becoming the mainstream machine translation method in the current industry. The mainstream model framework of neural machine translation is encoder -decoder model. The model uses two different neural networks as encoder and decoder respectively. The encoder is used to ICSP 2019 IOP Conf. Series: Journal of Physics: Conf. Series 1237 (2019) 052020 IOP Publishing doi:10.1088/1742-6596/1237/5/052020 2 read the source sentence, encode it into a vector of fixed dimension, and then the decoder reads the vector and generates the corresponding target language sequence. The neural networks used in the current mainstream encoder and decoder models are cyclic neural networks, their variants and convolutional neural networks. However, when the input sentence is long, the vector with fixed dimensions is difficult to store enough information, so the academic circle introduces an attention mechanism to solve this problem. The attention mechanism allows the decoder to look up words or fragments of the input sentence at any time, so it is not necessary for the intermediate vector to store all information. Then Ashish Vaswani proposed the Transformer model, which uses self -Attention, i.e. the model is completely dependent on the attention mechanism. Through the Attention layer, global connections can be captured one step at a time without obtaining step by step, thus leading to significant acceleration of training time and improvement of quality. Transformer includes other differences besides self -attention, including layer standardization of the whole model, multi-source attention mechanism, multi-head attention mechanism, residual connection and the use of feed-forward layer. This paper propose to introduce the multi-head attention, residual connection and feed-forward network into the cyclic neural network in the Transformer architecture, and the cyclic neural network can also be very close to the Transformer model in performance.  Related Work In 2014, Ilya Sutskever et al. proposed an end-to-end sequence learning method that uses a multi-layer LSTM to map an input sequence to a vector of fixed dimensions and then another depth LSTM to decode the target sequence of the vector [2]. This can be understood as In 2017, Jonas Gehring et al. propos\n"
     ]
    }
   ],
   "source": [
    "print(\"L1 Data Sample:\", L1_combined[:500] )\n",
    "print(\"L2 Data Sample:\", L2_combined[:500] )\n",
    "print(\"L3 Data Sample:\", L3_combined[:500] )\n",
    "print(\"L4 Data Sample:\", L4_combined[:500] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:39:26.053959Z",
     "start_time": "2025-04-24T14:39:24.227751Z"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# with open(\"Data/L1_ChildrenStories.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L1 = f.read()\n",
    "# with open(\"Data/L2_BookCorpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L2 = f.read()\n",
    "# with open(\"Data/L3_CNN_DailyMail.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L3 = f.read()\n",
    "# with open(\"Data/L4_S2ORC.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     L4 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T14:39:30.985039Z",
     "start_time": "2025-04-24T14:39:29.994292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/venv/lib/python3.12/site-packages (2.14.1)\n",
      "All levels cleaned and stored in *_cleaned variables!\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Define allowed character sets\n",
    "english_regex = r\"[a-zA-Z0-9\\s]\"                   # English letters, numbers, spaces\n",
    "math_symbols  = r\"[\\+\\-\\*/=<>∑∫√πθΣ∂∞]\"             # Add more math symbols if needed\n",
    "special_chars = r\"[\\.,!?;:'\\\"()\\[\\]{}#@%^&*_~]\"     # Common special characters\n",
    "\n",
    "# Function to clean a given text\n",
    "def clean_text(text):\n",
    "    return \"\".join(\n",
    "        c for c in text\n",
    "        if re.match(english_regex, c) or \n",
    "           re.match(math_symbols, c) or \n",
    "           re.match(special_chars, c) or \n",
    "           emoji.is_emoji(c)\n",
    "    )\n",
    "\n",
    "# Apply cleaning to all levels\n",
    "# L1_cleaned = clean_text(L1)\n",
    "# L2_cleaned = clean_text(L2)\n",
    "# L3_cleaned = clean_text(L3)\n",
    "# L4_cleaned = clean_text(L4)\n",
    "L1_cleaned = clean_text(L1_combined)\n",
    "L2_cleaned = clean_text(L2_combined)\n",
    "L3_cleaned = clean_text(L3_combined)\n",
    "L4_cleaned = clean_text(L4_combined)\n",
    "print(\"All levels cleaned and stored in *_cleaned variables!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419268673"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = L1_cleaned+L2_cleaned+L3_cleaned+L4_cleaned\n",
    "D = len(full_text)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "\t\n",
      " !\"#%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz{}~ ©®Σθπ  ™↔↘↪∂∑√∞∫▪▫▶☀☁☄☺♀♣♥♦♻⚙⚠⚡✏✔✨❤🌈🌍🌏🌐🌞🌟🌬🌱🌳🌸🌻🌿🍃🍇🍊🍞🍡🍭🎉🎮🎶🏠🏡🏰🏼🏽🐇🐛🐦🐰🐿👑👧👨💁💎💕💖💚💡💪💻💾📃📊📚📝📡📣📱📲🔒🔗🔬🕊🖥🗣😁😂😃😄😊😎😐😔😢😮🚀🚧🚯🤖🤩🤯🥗🥦🦉🦋🦝\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "token = full_text.encode(\"utf-8\")\n",
    "def get_stats(ids):\n",
    "    counts = {} # Creates an empty Dictionary\n",
    "    for pair in zip(ids,ids[1:]):\n",
    "        counts[pair] = counts.get(pair ,0)+1\n",
    "    return counts\n",
    "print(vocab_size)\n",
    "print(''.join(chars))\n",
    "stats = get_stats(token)\n",
    "top_pair = max(stats,key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, special_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train using iterator to save memory\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain_from_iterator(chunked_docs(), trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Check final vocab size\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual vocab size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mget_vocab_size())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "\n",
    "# Your cleaned text data\n",
    "docs = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]\n",
    "\n",
    "# Break large documents into smaller lines/chunks\n",
    "def chunked_docs():\n",
    "    for doc in docs:\n",
    "        # You can tweak the split here (e.g., '. ' or '\\n' or custom logic)\n",
    "        for line in doc.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield line\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "# Trainer with manageable vocab size\n",
    "trainer = BpeTrainer(vocab_size=2000, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "# Train using iterator to save memory\n",
    "tokenizer.train_from_iterator(chunked_docs(), trainer=trainer)\n",
    "\n",
    "# Check final vocab size\n",
    "print(\"Actual vocab size:\", tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(token_ids: list[int]) -> str:\n",
    "    return tokenizer.decode(token_ids)\n",
    "sample = \"math is beautiful ✨\"\n",
    "ids = encode(sample)\n",
    "print(\"→\", ids)\n",
    "print(\"←\", decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Like Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# ------------------ Hyperparameters ------------------\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# # ------------\n",
    "# # # hyperparameters for GPU\n",
    "# batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "# block_size = 512 # what is the maximum context length for predictions?\n",
    "# max_iters = 10000\n",
    "# eval_interval = 500\n",
    "# learning_rate = 3e-4\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embd = 384\n",
    "# n_head = 6\n",
    "# n_layer = 6\n",
    "# dropout = 0.2\n",
    "# # # ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# ------------------ BPE tokenizer functions (assumed defined) ------------------\n",
    "# encode(text) -> list of token ids\n",
    "# decode(ids)   -> string\n",
    "# merges, vocab, etc. already built above in your notebook\n",
    "\n",
    "# ------------------ Data loader for transformer ------------------\n",
    "def get_batch_transformer(data):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x  = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y  = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_transformer(model, train_data, val_data):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, data in zip(['train','val'], [train_data, val_data]):\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            X, Y = get_batch_transformer(data)\n",
    "            _, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))\n",
    "\n",
    "# ------------------ Transformer model classes ------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x); q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa    = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd  = FeedForward(n_embd)\n",
    "        self.ln1   = nn.LayerNorm(n_embd)\n",
    "        self.ln2   = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))   # Residual connection after self-attention\n",
    "        x = x + self.ffwd(self.ln2(x)) # Residual connection after feedforward\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f   = nn.LayerNorm(n_embd)\n",
    "        self.lm_head= nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets= targets.view(B*T)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _= self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs  = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# ------------------ Incremental Training Pipeline ------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def incremental_training_pipeline(level_texts, model, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    all_metrics = {\n",
    "        \"level\": [],\n",
    "        \"iters\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_ppl\": [],\n",
    "        \"val_ppl\": [],\n",
    "    }\n",
    "\n",
    "    for level_i, level_text in enumerate(level_texts, start=1):\n",
    "        print(f\"\\n=== LEVEL {level_i} TRAINING ===\")\n",
    "        data_ids   = torch.tensor(encode(level_text), dtype=torch.long).to(device)\n",
    "        n          = int(0.9 * data_ids.size(0))\n",
    "        train_data = data_ids[:n]\n",
    "        val_data   = data_ids[n:]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        for it in range(max_iters):\n",
    "            if it % eval_interval == 0:\n",
    "                losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "                ppl_train = compute_perplexity(losses['train'])\n",
    "                ppl_val   = compute_perplexity(losses['val'])\n",
    "\n",
    "                # Store for later plotting\n",
    "                all_metrics[\"level\"].append(level_i)\n",
    "                all_metrics[\"iters\"].append(it)\n",
    "                all_metrics[\"train_loss\"].append(losses['train'])\n",
    "                all_metrics[\"val_loss\"].append(losses['val'])\n",
    "                all_metrics[\"train_ppl\"].append(ppl_train.item())\n",
    "                all_metrics[\"val_ppl\"].append(ppl_val.item())\n",
    "\n",
    "                print(f\" it={it:4d} | train loss {losses['train']:.4f}  ppl {ppl_train:.2f}  | \"\n",
    "                      f\"val loss {losses['val']:.4f}  ppl {ppl_val:.2f}\")\n",
    "\n",
    "            xb, yb = get_batch_transformer(train_data)\n",
    "            logits, loss = model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # final evaluation\n",
    "        losses = estimate_loss_transformer(model, train_data, val_data)\n",
    "        ppl_train = compute_perplexity(losses['train'])\n",
    "        ppl_val   = compute_perplexity(losses['val'])\n",
    "\n",
    "        print(f\"\\n*** Level {level_i} complete.\")\n",
    "        print(f\"    Final train loss {losses['train']:.4f}, ppl {ppl_train:.2f}\")\n",
    "        print(f\"    Final   val loss {losses['val']:.4f}, ppl {ppl_val:.2f}\")\n",
    "\n",
    "        # sample generation\n",
    "        context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "        sample_ids = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "        print(\"    Sample:\", decode(sample_ids))\n",
    "\n",
    "        # save checkpoint\n",
    "        cp = os.path.join(checkpoint_dir, f\"model_level{level_i}.pt\")\n",
    "        torch.save(model.state_dict(), cp)\n",
    "        print(f\"    → Saved checkpoint: {cp}\")\n",
    "\n",
    "    return model, all_metrics\n",
    "\n",
    "\n",
    "# ------------------ Usage ------------------\n",
    "# Make sure: L1, L2, L3, L4_cleaned are already in your notebook\n",
    "level_texts = [L1_cleaned, L2_cleaned, L3_cleaned, L4_cleaned]\n",
    "\n",
    "# 'vocab_size' must match your BPE final vocabulary size (e.g. 276)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "\n",
    "model,all_metrics = incremental_training_pipeline(level_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, level in enumerate(level_texts, start=1):\n",
    "    D = len(level)\n",
    "    total_tokens_seen = block_size * batch_size * max_iters\n",
    "    epochs = total_tokens_seen / D\n",
    "    print(f\"Total Epochs for Level {i}: {epochs:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Shakespeare input prompt\n",
    "# # # Example Shakespearean prompt\n",
    "# prompt = \"Who is the President of USA \"\n",
    "#\n",
    "# # # Encode the prompt using your trained BPE tokenizer\n",
    "# encoded = encode(prompt)\n",
    "#\n",
    "# # # Truncate if the prompt is longer than block_size\n",
    "# if len(encoded) > block_size:\n",
    "#     encoded = encoded[-block_size:]\n",
    "#\n",
    "# # # Create context tensor\n",
    "# context = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "#\n",
    "# # # Generate continuation\n",
    "# generated_ids = model.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "#\n",
    "# # # Decode and print the generated text\n",
    "# print(decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(all_metrics):\n",
    "    levels = sorted(set(all_metrics[\"level\"]))\n",
    "    for level in levels:\n",
    "        # Get indices for this level\n",
    "        idxs = [i for i, l in enumerate(all_metrics[\"level\"]) if l == level]\n",
    "        iters = [all_metrics[\"iters\"][i] for i in idxs]\n",
    "        val_loss = [all_metrics[\"val_loss\"][i] for i in idxs]\n",
    "        val_ppl  = [all_metrics[\"val_ppl\"][i] for i in idxs]\n",
    "\n",
    "        # Create figure and first y-axis (for loss)\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.plot(iters, val_loss, color='blue', label='Validation Loss')\n",
    "        ax1.set_xlabel('Iterations')\n",
    "        ax1.set_ylabel('Validation Loss', color='blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Second y-axis for perplexity\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(iters, val_ppl, color='green', label='Validation Perplexity')\n",
    "        ax2.set_ylabel('Validation Perplexity', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "        plt.title(f'Level {level} - Validation Loss & Perplexity')\n",
    "        fig.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'transformer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7165480,
     "sourceId": 11438992,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212951,
     "sourceId": 11504366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7213450,
     "sourceId": 11504999,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
